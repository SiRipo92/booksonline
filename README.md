# Projet 2: Analyse de MarchÃ© avec Python

## Description

Ce projet vise Ã  dÃ©velopper une application Python qui automatise l'extraction, la transformation et le chargement (ETL) de donnÃ©es depuis le site [Books To Scrape](http://books.toscrape.com/). Le but est de rÃ©cupÃ©rer des informations produits, de les transformer en donnÃ©es exploitables et de les enregistrer dans des fichiers CSV. En parallÃ¨le, une version beta est mise en place pour suivre les prix des livres au moment de l'exÃ©cution.

## Objectifs d'Apprentissage

- **Programmation avec Python :** Automatiser la collecte de donnÃ©es depuis un site web.
- **Configuration d'un environnement de travail :** Mise en place dâ€™un environnement virtuel et gestion des dÃ©pendances.
- **Utilisation de Git et GitHub :** Suivi des versions avec des commits rÃ©guliers et messages descriptifs.
- **DÃ©veloppement d'une pipeline ETL :** Extraction, transformation et chargement des donnÃ©es dans des CSV.

## Mission Principale

DÃ©velopper un scraper capable de :
- Extraire les informations produits depuis le site Books To Scrape.
- Transformer ces donnÃ©es en fichiers CSV.
- TÃ©lÃ©charger et enregistrer les images associÃ©es aux produits.
- Mettre en place une version beta du systÃ¨me de surveillance des prix.

## Structure du Projet
```text
booksonline/
â”œâ”€â”€ main.py                       # Point d'entrÃ©e du projet
â”œâ”€â”€ requirements.txt              # DÃ©pendances Python Ã  installer
â”œâ”€â”€ assets/                       # Contient les fichiers gÃ©nÃ©rÃ©s
â”‚   â”œâ”€â”€ csv/                      # CSV exportÃ© contenant les donnÃ©es livres
â”‚   â”‚   â””â”€â”€ book_data.csv         # Copie 'master' de toutes les donnÃ©es extraites
â”‚   â”‚   â”œâ”€â”€ categories/           # CSVs par catÃ©gorie  
â”‚   â”‚   â”‚   â”œâ”€â”€ mystery.csv  
â”‚   â”‚   â”‚   â”œâ”€â”€ travel.csv  
â”‚   â”‚   â”‚   â””â”€â”€ â€¦ 
â”‚   â””â”€â”€ images/                   # Images tÃ©lÃ©chargÃ©es pour chaque livre
â”œâ”€â”€ utils/                        # Package utilitaire contenant les fonctions du scraper
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ book_scraper.py          # Scrape les infos de chaque livre, Ã©crit le CSV, supprime les doublons
â”‚   â””â”€â”€ category_scraper.py      # Scrape les catÃ©gories et les URL de chaque livre
```
### Comment les fichiers fonctionnent ensemble

#### ğŸ” `main.py`
- Coordonne tout le processus ETL.
- Appelle les fonctions des modules `utils/` dans l'ordre logique :
  1. `generate_categories_list()` â†’ rÃ©cupÃ¨re les catÃ©gories du site.
  2. `scrape_category()` â†’ rÃ©cupÃ¨re toutes les URLs de livres par catÃ©gorie.
  3. `scrape_book()` â†’ rÃ©cupÃ¨re les donnÃ©es de chaque livre (appel depuis `book_scraper.py`).
  4. `write_csv()` â†’ Ã©crit les donnÃ©es nettoyÃ©es dans un fichier CSV en supprimant les doublons.
  5. `download_book_images_from_csv()` â†’ tÃ©lÃ©charge les images selon les URL valides dans le CSV.

#### ğŸ§  `book_scraper.py`
- Fonction principale : `scrape_book(url)` :
  - Isole les donnÃ©es pertinentes (titre, prix, stock, etc.).
  - Nettoie et structure les rÃ©sultats dans un dictionnaire Python.
- `write_csv(book_info_list, file_path)` :
  - Supprime les doublons Ã  partir des `UPC`.
  - N'Ã©crit que les nouvelles entrÃ©es valides.
- `remove_csv_duplicate_rows(file_path)` :
  - Supprime toute rÃ©pÃ©tition ou ligne de type "header accidentel".

#### ğŸ“š `category_scraper.py`
- `generate_categories_list(base_url)` :
  - I/O: URL du site en entrÃ©e, dictionnaire `{nom_catÃ©gorie: url}` en sortie.
- `scrape_category(category_url)` :
  - RÃ©cupÃ¨re les pages d'une catÃ©gorie en gÃ©rant la pagination.
  - En sortie :
    - le nombre total de livres,
    - le nombre total de pages,
    - une liste dâ€™URL de chaque livre.
- `extract_book_urls(page_url)` :
  - RÃ©cupÃ¨re toutes les URL de livres listÃ©s dans une page de catÃ©gorie.

### Avantages de l'organisation modulaire
- **LisibilitÃ© amÃ©liorÃ©e** : chaque fichier a une mission claire (extraction, transformation, chargement).
- **Maintenance facilitÃ©e** : on peut modifier une fonction sans impacter le reste du systÃ¨me.
- **RÃ©utilisabilitÃ©** : fonctions comme `write_csv()` ou `scrape_book()` peuvent Ãªtre appelÃ©es ailleurs.
- **SÃ©paration des responsabilitÃ©s** : Ã©vite un fichier monolithique.


## Phases du Projet

### Phase 1 : Extraction d'une Page Produit

- **Objectif :**  
  Choisir une page produit sur Books To Scrape et extraire les informations suivantes :
  - `product_page_url`
  - `universal_product_code (upc)`
  - `title`
  - `price_including_tax`
  - `price_excluding_tax`
  - `number_available`
  - `product_description`
  - `category`
  - `review_rating`
  - `image_url`
- **Livrable :**  
  Un fichier CSV avec ces informations en en-tÃªtes de colonnes.

### Phase 2 : Extraction des DonnÃ©es d'une CatÃ©gorie

### Phase 1 : Extraction d'une Page Produit
- Extraire les donnÃ©es dÃ©taillÃ©es d'un livre donnÃ©.
- GÃ©nÃ©rer un dictionnaire des infos du livre.

### Phase 2 : Extraction d'une CatÃ©gorie
- Scraper toutes les pages d'une catÃ©gorie et leurs produits.
- GÃ©rer la pagination.

### Phase 3 : Extraction Globale
- Automatiser le scraping de toutes les catÃ©gories du site.
- AgrÃ©ger les donnÃ©es dans un CSV.

### Phase 4 : TÃ©lÃ©chargement des Images
- Identifier et tÃ©lÃ©charger toutes les images depuis les URL extraites.
- VÃ©rifier quâ€™elles respectent les extensions autorisÃ©es (`.jpg`, `.jpeg`, `.png`, `.svg`, `.gif`, `.webp`).
- RÃ©essayer les tÃ©lÃ©chargements Ã©chouÃ©s avec un `timeout` et `retry`.

## Installation et Configuration

### PrÃ©requis

- **Python 3.12**
- **Git** pour le contrÃ´le de version

### Installation des DÃ©pendances

1. **CrÃ©ez un environnement virtuel** (recommandÃ©) :
   ```bash
   python3 -m venv env
   ```
2. **Activez l'environnement virtuel**
   - Sur Windows
   ```
   .\env\Scripts\activate
   ```
   - Sur MacOs
   ```
   source env/bin/activate
3. **Installez les dÃ©pendances** depuis <code>requirements.txt</code>
```
pip install -r requirements.txt
```
    
*Note - ceci n'est pas inclus dans ce repository*

## Utilisation

ExÃ©cuter le projet avec :
```bash
python3 main.py
```

Ce script :
- scrape toutes les catÃ©gories,
- tÃ©lÃ©charge toutes les informations produit,
- nettoie les doublons,
- Ã©crit le CSV,
- tÃ©lÃ©charge les images si elles sont valides.

### Remarques
- **Surveillance en Temps RÃ©el:** Ce projet ne rÃ©alise pas une surveillance continue des prix, mais un relevÃ© des prix au moment de l'exÃ©cution.
- **Fichiers Ã  Exclure:** Les fichiers gÃ©nÃ©rÃ©s (CSV, images, dossier de l'environnement virtuel dans le .gitignore)
- **Documentation:** Des commentaires dans le code et ce README fournissent une vue d'ensemble pour faciliter la comprÃ©hension et la maintenance du projet.

## Author
Sierra Ripoche - Junior web developer

## License

Ce projet est licenciÃ© sous la Licence Apache, Version 2.0.  
Voir le fichier [LICENSE.md](LICENSE.md) pour plus de dÃ©tails.